import torch
import numpy as np
import torchvision
import Swarm_Observer as SO 
import Adversarial_Observation as AO
import os
from util import *
from umap import UMAP
import matplotlib.pyplot as plt
from util import *
from sklearn.cluster import KMeans

# global variables
label = 0
initial = 8
epochs = 5
points = 30

import torch

def cost_func(model, x):
    """
    Compute the cost (prediction score) for a given input data point using a pre-trained model.

    Args:
        model (torch.nn.Module): Pre-trained PyTorch model.
        x (numpy.ndarray): Input data as a numpy array.
            Shape: (channels, height, width)

    Returns:
        cost (float): Prediction score (cost) for the input data.
    """
    global label
    model.eval()  # Set the model to evaluation mode

    # Convert the input data to a PyTorch tensor and reshape it to match the model's input shape
    x = torch.tensor(x.reshape(1, 1, 28, 28)).to(torch.float32)

    # Normalize the data to the range of 0-1
    x = (x - x.min()) / (x.max() - x.min())

    # Forward pass the input data through the model
    output = model(x)

    # Get the prediction score for the target label
    pred = output[0][label]

    # Return the prediction score as a float
    return pred.item()


def umap_data(dataloader):
    """
    Generate UMAP embeddings for the data in a given dataloader.

    Args:
        dataloader (torch.utils.data.DataLoader): DataLoader containing the input data.

    Returns:
        umapLearner (umap.UMAP): UMAP object fitted on the flattened data.
        dataDic (dict): Dictionary mapping target labels to their corresponding data points.
    """
    dataReduce = []
    dataDic = {}

    # Iterate over the dataloader and extract data points
    for batch_idx, (data, target) in enumerate(dataloader):
        if batch_idx > 10:
            break
        for i in range(data.shape[0]):
            dataReduce.append(data[i].numpy())
            if target[i].item() not in dataDic.keys():
                dataDic[target[i].item()] = []
            dataDic[target[i].item()].append(data[i].numpy())

    # Convert the data to numpy arrays and concatenate them
    dataReduce = np.array(dataReduce)
    dataReduce = np.concatenate(dataReduce, axis=0)

    # Flatten the data
    flattened_data = dataReduce.reshape(-1, 28*28)

    # Apply UMAP to obtain embeddings
    umapLearner = UMAP(n_components=2)
    print("Fitting UMAP")
    umapLearner.fit(flattened_data)
    print("Done Fitting UMAP")

    return umapLearner, dataDic
    

def plotData(data, umap, title, saveName):
    """
    Plot the data points using UMAP embeddings.

    Args:
        data (dict): Dictionary mapping target labels to their corresponding data points.
        umap (umap.UMAP): UMAP object used for transforming the data points.
        title (str): Title of the plot.
        saveName (str): File name to save the plot.

    Returns:
        None
    """
    plt.clf()
    global label
    global initial

    for key in data.keys():
        # Apply UMAP transformation to the data points
        transformed = umap.transform(np.array(data[key]).reshape(-1, 28*28))

        # Plot the transformed data
        if key == "Attack":
            # If the key is "Attack", plot black "x" markers
            plt.scatter(transformed[:, 0], transformed[:, 1], label=key, marker='x', c='black')
        else:
            plt.scatter(transformed[:, 0], transformed[:, 1], label=key)

    plt.legend()
    plt.title(title)
    plt.savefig(saveName)
    plt.close()
    plt.clf()

def plotPSO(points, step, model, runName):
    """
    Plot the PSO (Particle Swarm Optimization) results.

    Args:
        points (list): List of points generated by the PSO algorithm.
        step (int): Current step or iteration number.
        model (torch.nn.Module): Pre-trained PyTorch model.
        runName (str): Name of the PSO run.

    Returns:
        None
    """
    global label

    acts = AO.Attacks.activation_map(torch.tensor(points).to(torch.float32).squeeze(1), model, normalize=False)

    for img, act, index in zip(points, acts, range(len(points))):
        fig, ax = plt.subplots(1, 2)
        img = img.reshape(28, 28)
        act = act.reshape(28, 28)

        # Create directories to save the results
        os.makedirs(f"./APSO/{runName}/step_{step}_index_{index}", exist_ok=True)
        np.save(f"./APSO/{runName}/step_{step}_index_{index}/img.npy", img)
        np.save(f"./APSO/{runName}/step_{step}_index_{index}/act.npy", act)

        # Plot the original image and activation map
        ax[0].imshow(img, cmap='gray')
        ax[0].axis('off')
        ax[0].set_title(f"Confidence of {label}: {np.round(cost_func(model, img), 5)}")
        ax[1].imshow(act, cmap='jet')
        ax[1].axis('off')
        ax[1].set_title("Activation Map")

        # Add colorbars to both plots
        fig.colorbar(ax[0].imshow(img, cmap='gray'), ax=ax[0], fraction=0.046, pad=0.04)
        fig.colorbar(ax[1].imshow(act, cmap='jet'), ax=ax[1], fraction=0.046, pad=0.04)

        # Save the figure
        plt.savefig(f"./APSO/{runName}/step_{step}_index_{index}/img.png", bbox_inches='tight', pad_inches=0)

        # Close and clear the figure
        plt.close()
        plt.clf()
        

def getPositions(APSO):
    return [particle.position_i for particle in APSO.swarm]



def plotBest(APSO, model, runName, epoch):
    """
    Plot the best point and its activation map from APSO (Adaptive Particle Swarm Optimization) results.

    Args:
        APSO (object): APSO object containing the results of the optimization.
        model (torch.nn.Module): Pre-trained PyTorch model.
        runName (str): Name of the APSO run.
        epoch (int): Current epoch or iteration number.

    Returns:
        None
    """
    # Get the best point from APSO
    bestPoint = APSO.pos_best_g
    bestPoint = bestPoint.reshape(28, 28)

    # Compute the activation map for the best point
    act = AO.Attacks.activation_map(torch.tensor(bestPoint.reshape(1, 1, 28, 28)).to(torch.float32), model, normalize=False).reshape(28, 28)

    # Compute the confidence score of the best point
    confidence = np.round(cost_func(model, bestPoint), 5)

    # Create a figure with two subplots and a colorbar subplot
    fig, (ax1, ax2, cax) = plt.subplots(1, 3, figsize=(12, 4), gridspec_kw={"width_ratios": [1, 1, 0.05]})

    # Plot the best point
    im1 = ax1.imshow(bestPoint, cmap='gray')
    ax1.axis('off')
    ax1.set_title(f"Confidence of {label}: {confidence}")

    # Plot the activation map
    im2 = ax2.imshow(act, cmap='jet')
    ax2.axis('off')
    ax2.set_title("Activation Map")

    # Add a colorbar for the activation map
    cbar = plt.colorbar(im2, cax=cax)
    cbar.ax.set_ylabel("Activation")

    # Adjust the layout to make room for the colorbar
    plt.tight_layout(rect=[0, 0, 0.9, 1])

    # Save the figure
    plt.savefig(f"./APSO/{runName}/best_{epoch}.png", bbox_inches='tight', pad_inches=0)

    # Close and clear the figure
    plt.close()
    plt.clf()

    # Create a directory to save the best point and its activation map
    os.makedirs(f"./APSO/{runName}/bestData", exist_ok=True)

    # Save the best point and its activation map as .npy files
    np.save(f"./APSO/{runName}/bestData/{epoch}.npy", bestPoint)
    np.save(f"./APSO/{runName}/bestData/{epoch}_act.npy", act)

def runAPSO(points, epochs, model, cost_func, dataDic, umap, runName):
    """
    Run the Adaptive Particle Swarm Optimization (APSO) algorithm.

    Args:
        points (np.ndarray): Initial set of points for optimization.
        epochs (int): Number of optimization epochs.
        model (torch.nn.Module): Pre-trained PyTorch model.
        cost_func (function): Cost function to optimize.
        dataDic (dict): Dictionary containing the data points.
        umap (object): UMAP object for data visualization.
        runName (str): Name of the APSO run.

    Returns:
        np.ndarray: Final positions of the particles after optimization.
    """
    os.makedirs(f"./APSO/{runName}", exist_ok=True)

    # Initialize the swarm
    APSO = SO.Swarm.PSO(torch.tensor(points).reshape(-1, 1, 1, 28, 28), cost_func, model, w=0.8, c1=0.5, c2=0.5)
    positions = getPositions(APSO)
    plotPSO(positions, 0, model, runName)

    # Run the swarm optimization
    for epoch in range(1, epochs + 1):
        print(f"Epoch: {epoch}")
        for particle in APSO.swarm:
            # Normalize the particle position to be between 0 and 1
            particle.position_i = np.clip(particle.position_i, 0, 1)
            # Normalize the particle velocity to be between -1 and 1
            particle.velocity_i = np.clip(particle.velocity_i, -1, 1)
            APSO.step()

        # Plot the swarm positions
        positions = getPositions(APSO)
        plotPSO(positions, epoch, model, runName)
        dataDic["Attack"] = positions
        plotData(dataDic, umap, 'UMAP of MNIST Data with Attack', f'./APSO/{runName}/umap{epoch}.png')
        plotBest(APSO, model, runName, epoch)

    # Save the simulation history
    APSO.save_history(f"./APSO/{runName}/APSO_simulation.csv")

    return getPositions(APSO)

def main():
    global label
    global initial
    global points

    seedEverything(1024)
    clusters = 2
    train_loader, test_loader = load_MNIST_data()
    model = build_MNIST_Model()
    model.load_state_dict(torch.load('MNIST_cnn.pt'))
    
    umap, dataDic = umap_data(train_loader)
    
    # APSO for sparce random noise to another
    dataDic["Attack"] = []
    for i in range(points):
        data = np.random.rand(1,28,28)
        #  set 90% of the data to 0
        data[data < .9] = 0
        dataDic["Attack"].append(data)
    initalPoints = dataDic["Attack"]
    positions = runAPSO(initalPoints, epochs, model, cost_func, dataDic, umap, f"MNIST_sparce_noise_{label}")
    plot_clusters(positions, model, clusters, f"MNIST_sparce_noise_{label}")

    # APSO for low random noise to another
    dataDic["Attack"] = []
    for i in range(points):
        dataDic["Attack"].append(np.random.rand(1,28,28)*.001)
    initalPoints = dataDic["Attack"]
    positions = runAPSO(initalPoints, epochs, model, cost_func, dataDic, umap, f"MNIST_low_noise_{label}")
    plot_clusters(positions, model, clusters, f"MNIST_low_noise_{label}")

    # APSO for all labels to another
    #  get the first 10 images of each label
    dataDic["Attack"] = []
    for key in dataDic.keys():
        if key != "Attack" and label != key:
            dataDic["Attack"] += dataDic[key][:points//10]
    initalPoints = dataDic["Attack"]
    positions = runAPSO(initalPoints, epochs, model, cost_func, dataDic, umap, f"MNIST_all_{label}")
    plot_clusters(positions, model, clusters,f"MNIST_all_{label}")

    # APSO for random noise to another
    dataDic["Attack"] = []
    for i in range(points):
        dataDic["Attack"].append(np.random.rand(1,28,28))
    initalPoints = dataDic["Attack"]
    positions = runAPSO(initalPoints, epochs, model, cost_func, dataDic, umap, f"MNIST_noise_{label}")
    plot_clusters(positions, model, clusters, f"MNIST_noise_{label}")

    
    # APSO for one label to another
    dataDic["Attack"] = dataDic[initial][:points]
    initalPoints = dataDic[initial][:points]
    positions = runAPSO(initalPoints, epochs, model, cost_func, dataDic, umap, f"MNIST_{initial}_{label}")
    plot_clusters(positions, model, clusters,f"MNIST_{initial}_{label}")


def plot_clusters(positions, model, clusters, runName):
    positions = np.array(positions)
    
    positions = positions.reshape(-1,1*28*28)
    os.makedirs(f"./APSO_Cluster/{runName}/umap", exist_ok=True)
    # save the positions
    np.save(f"./APSO_Cluster/{runName}/positions.npy", positions)
    
    # cluster the positions using sklearn 
    kmeans = KMeans(n_clusters=clusters, random_state=0).fit(positions)
    
    # for cluser in kmeans.cluster_centers_, plot the cluster average and the activation map
    cluster_index = 0
    for cluster in kmeans.cluster_centers_:
        cluster = cluster.reshape(28,28)
        # normalizer between 0 and 1
        cluster = (cluster - cluster.min())/(cluster.max() - cluster.min())
        plt.imshow(cluster, cmap='gray')
        conf = cost_func(model, torch.tensor(cluster.reshape(1,1,28,28)))
        plt.title("Average of Cluster with Confidence: " + str(np.round(conf,5)))
        plt.savefig(f"./APSO_Cluster/{runName}/umap/cluster{cluster_index}.png")
        plt.close()
        plt.clf()

        act = AO.Attacks.activation_map(torch.tensor(cluster.reshape(1,1,28,28)).to(torch.float32), model)
        act = act.reshape(28,28)
        # normalizer between 0 and 1
        act = (act - act.min())/(act.max() - act.min() + .0000001)
        plt.imshow(act, cmap='jet')
        plt.colorbar()
        plt.savefig(f"./APSO_Cluster/{runName}/umap/cluster{cluster_index}_act.png")
        plt.close()
        plt.clf()
        cluster_index += 1
    
if __name__ == '__main__':
    main()